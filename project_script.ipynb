{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzUzkaru7-DE",
        "outputId": "61afc74f-c2a8-41d3-ee0b-567fbaf20929"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/oxford_pets\n",
            "--2025-11-22 14:20:40--  https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://thor.robots.ox.ac.uk/pets/images.tar.gz [following]\n",
            "--2025-11-22 14:20:41--  https://thor.robots.ox.ac.uk/pets/images.tar.gz\n",
            "Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98\n",
            "Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 791918971 (755M) [application/octet-stream]\n",
            "Saving to: ‘images.tar.gz’\n",
            "\n",
            "images.tar.gz       100%[===================>] 755.23M  16.6MB/s    in 40s     \n",
            "\n",
            "2025-11-22 14:21:22 (18.7 MB/s) - ‘images.tar.gz’ saved [791918971/791918971]\n",
            "\n",
            "--2025-11-22 14:21:22--  https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://thor.robots.ox.ac.uk/pets/annotations.tar.gz [following]\n",
            "--2025-11-22 14:21:23--  https://thor.robots.ox.ac.uk/pets/annotations.tar.gz\n",
            "Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98\n",
            "Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19173078 (18M) [application/octet-stream]\n",
            "Saving to: ‘annotations.tar.gz’\n",
            "\n",
            "annotations.tar.gz  100%[===================>]  18.28M  9.92MB/s    in 1.8s    \n",
            "\n",
            "2025-11-22 14:21:26 (9.92 MB/s) - ‘annotations.tar.gz’ saved [19173078/19173078]\n",
            "\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p oxford_pets\n",
        "%cd oxford_pets\n",
        "\n",
        "!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "\n",
        "!tar -xzf images.tar.gz\n",
        "!tar -xzf annotations.tar.gz\n",
        "\n",
        "%cd ..\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_83U7n88LeO",
        "outputId": "b0c6a264-528c-47b9-fe31-7d676a699661"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num classes: 37\n",
            "Sample classes: ['abyssinian', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'bengal', 'birman', 'bombay', 'boxer', 'british_shorthair']\n",
            "Total images: 7390\n",
            "Sample item: oxford_pets/images/Abyssinian_1.jpg  label= 0 abyssinian\n",
            "\n",
            "Example prompts:\n",
            "0 a photo of a abyssinian\n",
            "1 a photo of a american bulldog\n",
            "2 a photo of a american pit bull terrier\n",
            "3 a photo of a basset hound\n",
            "4 a photo of a beagle\n",
            "Text embedding shape: torch.Size([37, 512])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/7390 [00:00<33:10,  3.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DEBUG: Abyssinian_1.jpg\n",
            " TRUE: 0 abyssinian\n",
            "  PRED 0 abyssinian 0.3889784812927246\n",
            "  PRED 33 sphynx 0.30687782168388367\n",
            "  PRED 11 egyptian_mau 0.29848116636276245\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 2/7390 [00:00<30:37,  4.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DEBUG: Abyssinian_10.jpg\n",
            " TRUE: 0 abyssinian\n",
            "  PRED 0 abyssinian 0.2773655354976654\n",
            "  PRED 11 egyptian_mau 0.2773251533508301\n",
            "  PRED 33 sphynx 0.26471394300460815\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 3/7390 [00:00<30:44,  4.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DEBUG: Abyssinian_100.jpg\n",
            " TRUE: 0 abyssinian\n",
            "  PRED 0 abyssinian 0.2988705635070801\n",
            "  PRED 9 british_shorthair 0.2708992063999176\n",
            "  PRED 5 bengal 0.24696201086044312\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7390/7390 [33:17<00:00,  3.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Zero-shot accuracy: 0.8220568335588633\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "RAW_IMG_DIR = \"oxford_pets/images\"   \n",
        "\n",
        "classnames = sorted({\n",
        "    \"_\".join(fname.split(\"_\")[:-1]).lower()\n",
        "    for fname in os.listdir(RAW_IMG_DIR)\n",
        "    if fname.lower().endswith(\".jpg\")\n",
        "})\n",
        "print(\"Num classes:\", len(classnames))\n",
        "print(\"Sample classes:\", classnames[:10])\n",
        "\n",
        "items = []\n",
        "for fname in sorted(os.listdir(RAW_IMG_DIR)):\n",
        "    if fname.lower().endswith(\".jpg\"):\n",
        "        breed = \"_\".join(fname.split(\"_\")[:-1]).lower()\n",
        "        label = classnames.index(breed)\n",
        "        items.append((os.path.join(RAW_IMG_DIR, fname), label))\n",
        "\n",
        "print(\"Total images:\", len(items))\n",
        "print(\"Sample item:\", items[0][0], \" label=\", items[0][1], classnames[items[0][1]])\n",
        "\n",
        "\n",
        "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "clip_model = CLIPModel.from_pretrained(model_name).to(device)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "tokenizer = CLIPTokenizer.from_pretrained(model_name)\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_images(pil_images):\n",
        "    inputs = processor(images=pil_images, return_tensors=\"pt\").to(device)\n",
        "    feats = clip_model.get_image_features(**inputs)\n",
        "    feats = feats / feats.norm(dim=-1, keepdim=True)\n",
        "    return feats.cpu()\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_text(prompts):\n",
        "    tokens = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(device)\n",
        "    feats = clip_model.get_text_features(**tokens)    \n",
        "    feats = feats / feats.norm(dim=-1, keepdim=True)\n",
        "    return feats.cpu()\n",
        "\n",
        "tmpl = \"a photo of a {}\"\n",
        "prompts = [tmpl.format(c.replace(\"_\", \" \")) for c in classnames]\n",
        "\n",
        "print(\"\\nExample prompts:\")\n",
        "for i in range(5):\n",
        "    print(i, prompts[i])\n",
        "\n",
        "text_emb = encode_text(prompts)\n",
        "print(\"Text embedding shape:\", text_emb.shape)\n",
        "\n",
        "def zero_shot_eval(items, text_emb, debug=3):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for idx, (imgpath, label) in enumerate(tqdm(items)):\n",
        "        img = Image.open(imgpath).convert(\"RGB\")\n",
        "        img_emb = encode_images([img])[0].numpy()\n",
        "        sims = img_emb @ text_emb.T.numpy()\n",
        "        pred = int(sims.argmax())\n",
        "\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "\n",
        "        if idx < debug:\n",
        "            print(\"\\nDEBUG:\", os.path.basename(imgpath))\n",
        "            print(\" TRUE:\", label, classnames[label])\n",
        "            top3 = sims.argsort()[-3:][::-1]\n",
        "            for r in top3:\n",
        "                print(\"  PRED\", r, classnames[r], float(sims[r]))\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "acc = zero_shot_eval(items, text_emb)\n",
        "print(\"\\nZero-shot accuracy:\", acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "2wOtYpAcUPJK",
        "outputId": "5a030a41-a43b-4906-9939-fe343c451900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.12/dist-packages (from openai==0.28) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from openai==0.28) (3.13.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->openai==0.28) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->openai==0.28) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->openai==0.28) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->openai==0.28) (2025.11.12)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (1.22.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->openai==0.28) (4.15.0)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.109.1\n",
            "    Uninstalling openai-1.109.1:\n",
            "      Successfully uninstalled openai-1.109.1\n",
            "Successfully installed openai-0.28.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "8385e160e8b34c22a8c6a25c3b6f3288",
              "pip_warning": {
                "packages": [
                  "openai"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zy-Ks5zyTiuf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "J1XAjVd3RDd9",
        "outputId": "c6f8d6b2-1441-4010-d7fd-b5e9fef47b6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using 37 classnames (first 8): ['abyssinian', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'bengal', 'birman', 'bombay']\n",
            "OpenAI request failed (attempt 1 ): You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n",
            "OpenAI request failed (attempt 2 ): You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n",
            "OpenAI request failed (attempt 3 ): You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n",
            "OpenAI request failed (attempt 4 ): You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "OpenAI calls failed repeatedly.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2642434683.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mDo\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madd\u001b[0m \u001b[0many\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \"\"\"\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat_with_gpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;31m# crude extraction: evaluate as python literal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2642434683.py\u001b[0m in \u001b[0;36mchat_with_gpt\u001b[0;34m(system_prompt, user_prompt, model, temperature, max_tokens)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OpenAI request failed (attempt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"):\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OpenAI calls failed repeatedly.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# ==========================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: OpenAI calls failed repeatedly."
          ]
        }
      ],
      "source": [
        "import os, json, time, math\n",
        "from pathlib import Path\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") or \"\"  \n",
        "if not OPENAI_API_KEY:\n",
        "    raise RuntimeError(\"Set OPENAI_API_KEY in your environment or paste it into OPENAI_API_KEY variable in this cell.\")\n",
        "\n",
        "LLM_NAME = \"gpt-5.1\"\n",
        "TEMPERATURE = 0.0\n",
        "MAX_TOKENS = 1024\n",
        "\n",
        "CACHE_DIR = Path(\"vdt_gpt_cache\")\n",
        "CACHE_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "classnames = list(dict.fromkeys(classnames))  \n",
        "print(\"Using\", len(classnames), \"classnames (first 8):\", classnames[:8])\n",
        "\n",
        "import openai\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "def chat_with_gpt(system_prompt, user_prompt, model=LLM_NAME, temperature=TEMPERATURE, max_tokens=MAX_TOKENS):\n",
        "    for attempt in range(4):\n",
        "        try:\n",
        "            resp = openai.ChatCompletion.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                    {\"role\":\"system\",\"content\":system_prompt},\n",
        "                    {\"role\":\"user\",\"content\":user_prompt}\n",
        "                ],\n",
        "                temperature=temperature,\n",
        "                max_tokens=max_tokens\n",
        "            )\n",
        "            return resp[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "        except Exception as e:\n",
        "            print(\"OpenAI request failed (attempt\", attempt+1, \"):\", str(e))\n",
        "            time.sleep(2 ** attempt)\n",
        "    raise RuntimeError(\"OpenAI calls failed repeatedly.\")\n",
        "\n",
        "ATTR_CACHE = CACHE_DIR / \"attributes.json\"\n",
        "if ATTR_CACHE.exists():\n",
        "    attributes = json.loads(ATTR_CACHE.read_text())\n",
        "    print(\"Loaded attributes from cache:\", attributes)\n",
        "else:\n",
        "    system = \"You are ChatGPT. Return only a Python list of strings (no prose).\"\n",
        "    user = f\"\"\"\n",
        "We will create visually descriptive attributes for the Oxford-IIIT Pets dataset to build visual descriptors (VDT).\n",
        "Using only the following 37 breed names (exactly): {classnames}\n",
        "\n",
        "Return exactly 20 short attribute names (one- or few-word each) that are visually observable in photos and useful to distinguish these pet breeds.\n",
        "Examples of attribute categories: coat color, coat pattern, fur length, ear shape, tail shape, face flatness, body size, etc.\n",
        "\n",
        "Return the output as a Python list literal like:\n",
        "[\"attribute1\", \"attribute2\", ...]\n",
        "Do not add any explanation text.\n",
        "\"\"\"\n",
        "    raw = chat_with_gpt(system, user)\n",
        "    try:\n",
        "        attributes = eval(raw, {})\n",
        "        assert isinstance(attributes, (list,tuple))\n",
        "        attributes = [str(a).strip() for a in attributes][:20]\n",
        "    except Exception:\n",
        "        attributes = [line.strip(\"- \").strip() for line in raw.splitlines() if line.strip()]\n",
        "        attributes = attributes[:20]\n",
        "    print(\"Attributes:\", attributes)\n",
        "    with open(ATTR_CACHE, \"w\") as f:\n",
        "        json.dump(attributes, f, indent=2)\n",
        "\n",
        "VDT_CACHE = CACHE_DIR / \"vdt_sentences.json\"\n",
        "vdt = {}\n",
        "if VDT_CACHE.exists():\n",
        "    vdt = json.loads(VDT_CACHE.read_text())\n",
        "    print(\"Loaded per-class VDT from cache with\", len(vdt), \"classes.\")\n",
        "else:\n",
        "    system = \"You are ChatGPT. Return only a valid Python dictionary (no extra text).\"\n",
        "    for cls in classnames:\n",
        "        cls_safe = cls.replace(\"_\", \" \")\n",
        "        cache_file = CACHE_DIR / f\"vdt_{cls}.json\"\n",
        "        if cache_file.exists():\n",
        "            v = json.loads(cache_file.read_text())\n",
        "            vdt[cls] = v\n",
        "            print(\"Loaded cached\", cls)\n",
        "            continue\n",
        "\n",
        "        user = f\"\"\"\n",
        "Using the attribute list: {attributes}\n",
        "\n",
        "For the class \"{cls_safe}\", produce EXACTLY {len(attributes)} short sentences (one sentence per attribute), describing how that attribute appears in this breed in photographs.\n",
        "- Each sentence should be 6-20 words.\n",
        "- Focus only on visual, image-observable features (color, patterns, ear/tail shape, fur length, body size, face, posture).\n",
        "- Return a JSON array (Python list) of strings, with each sentence corresponding to the attribute at the same index in the attribute list.\n",
        "Example expected output:\n",
        "[\"Short dense coat with spotted pattern.\", \"Fur color is orange with black rosettes.\", ...]\n",
        "\n",
        "Return only the JSON list of strings. No commentary. If an attribute is not applicable, write a short sentence like \"Not visually distinctive.\"\n",
        "\"\"\"\n",
        "        raw = chat_with_gpt(system, user)\n",
        "\n",
        "        try:\n",
        "            arr = eval(raw, {})\n",
        "            if isinstance(arr, str):\n",
        "                import json as _json\n",
        "                arr = _json.loads(arr)\n",
        "            assert isinstance(arr, (list,tuple))\n",
        "            arr = [str(s).strip() for s in arr]\n",
        "        except Exception:\n",
        "            try:\n",
        "                import json as _json\n",
        "                arr = _json.loads(raw)\n",
        "            except Exception:\n",
        "                arr = [line.strip(\"- \").strip() for line in raw.splitlines() if line.strip()]\n",
        "        if len(arr) != len(attributes):\n",
        "            if len(arr) < len(attributes):\n",
        "                arr += [\"Not visually distinctive.\"] * (len(attributes)-len(arr))\n",
        "            else:\n",
        "                arr = arr[:len(attributes)]\n",
        "        vdt[cls] = arr\n",
        "        cache_file.write_text(json.dumps(arr, indent=2))\n",
        "        print(\"Saved VDT for\", cls)\n",
        "        time.sleep(0.5)\n",
        "    VDT_CACHE.write_text(json.dumps(vdt, indent=2))\n",
        "\n",
        "print(\"VDT generation done. Classes collected:\", len(vdt))\n",
        "\n",
        "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name = \"openai/clip-vit-base-patch32\"\n",
        "clip_model = CLIPModel.from_pretrained(model_name).to(device)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "tokenizer = CLIPTokenizer.from_pretrained(model_name)\n",
        "\n",
        "import torch\n",
        "@torch.no_grad()\n",
        "def encode_texts_clips(prompts):\n",
        "    B = 64\n",
        "    all_emb = []\n",
        "    for i in range(0, len(prompts), B):\n",
        "        batch = prompts[i:i+B]\n",
        "        tokens = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "        feats = clip_model.get_text_features(**tokens)\n",
        "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
        "        all_emb.append(feats.cpu())\n",
        "    return torch.cat(all_emb, dim=0)  \n",
        "\n",
        "PROMPT_TEMPLATE = \"a photo of a {classname}. {sentence}\"\n",
        "all_prompts = []\n",
        "idx_to_pair = []  \n",
        "for cls in classnames:\n",
        "    sentences = vdt[cls]\n",
        "    for j, sent in enumerate(sentences):\n",
        "        p = PROMPT_TEMPLATE.format(classname=cls.replace(\"_\", \" \"), sentence=sent)\n",
        "        all_prompts.append(p)\n",
        "        idx_to_pair.append((cls, j))\n",
        "\n",
        "print(\"Total prompts:\", len(all_prompts), \"-> should be len(classes) * len(attributes) =\", len(classnames)*len(attributes))\n",
        "\n",
        "text_embeddings = encode_texts_clips(all_prompts)  \n",
        "print(\"Encoded text embeddings shape:\", text_embeddings.shape)\n",
        "\n",
        "import numpy as np\n",
        "D = text_embeddings.shape[1]\n",
        "class_embs = {cls: None for cls in classnames}\n",
        "counts = {cls: 0 for cls in classnames}\n",
        "for i, (cls, j) in enumerate(idx_to_pair):\n",
        "    vec = text_embeddings[i].numpy()\n",
        "    if class_embs[cls] is None:\n",
        "        class_embs[cls] = vec.copy()\n",
        "    else:\n",
        "        class_embs[cls] += vec\n",
        "    counts[cls] += 1\n",
        "\n",
        "for cls in class_embs:\n",
        "    class_embs[cls] = class_embs[cls] / max(1, counts[cls])\n",
        "    norm = np.linalg.norm(class_embs[cls])\n",
        "    if norm > 0:\n",
        "        class_embs[cls] = class_embs[cls] / norm\n",
        "\n",
        "import torch\n",
        "text_emb_vdt = torch.stack([torch.from_numpy(class_embs[c]) for c in classnames], dim=0)  \n",
        "print(\"Aggregated class embedding matrix shape:\", text_emb_vdt.shape)\n",
        "\n",
        "from PIL import Image\n",
        "@torch.no_grad()\n",
        "def encode_images_batch(pil_images):\n",
        "    inputs = processor(images=pil_images, return_tensors=\"pt\").to(device)\n",
        "    feats = clip_model.get_image_features(**inputs)\n",
        "    feats = feats / feats.norm(dim=-1, keepdim=True)\n",
        "    return feats.cpu()\n",
        "\n",
        "def zero_shot_eval_with_emb(items, text_emb_matrix, max_images=None, debug=3):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for idx, (imgpath, label) in enumerate(items[:max_images] if max_images else items):\n",
        "        img = Image.open(imgpath).convert(\"RGB\")\n",
        "        img_emb = encode_images_batch([img])[0].numpy()\n",
        "        sims = img_emb @ text_emb_matrix.numpy().T  \n",
        "        pred = int(sims.argmax())\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        if idx < debug:\n",
        "            top3 = sims.argsort()[-3:][::-1]\n",
        "            print(\"DBG:\", Path(imgpath).name, \"true:\", classnames[label])\n",
        "            for r in top3:\n",
        "                print(\"   pred\", r, classnames[r], float(sims[r]))\n",
        "    return correct / total\n",
        "\n",
        "try:\n",
        "    baseline_text_emb  \n",
        "except NameError:\n",
        "    baseline_prompts = [f\"a photo of a {c.replace('_',' ')}\" for c in classnames]\n",
        "    baseline_text_emb = encode_texts_clips(baseline_prompts)\n",
        "\n",
        "print(\"Evaluating baseline (simple prompt) on 500 images (quick) ...\")\n",
        "acc_baseline = zero_shot_eval_with_emb(items, baseline_text_emb, max_images=500, debug=2)\n",
        "print(\"Baseline (500) accuracy:\", acc_baseline)\n",
        "\n",
        "print(\"Evaluating VDT-GPT aggregated embeddings on same subset ...\")\n",
        "acc_vdt = zero_shot_eval_with_emb(items, text_emb_vdt, max_images=500, debug=2)\n",
        "print(\"VDT-GPT (500) accuracy:\", acc_vdt)\n",
        "\n",
        "with open(CACHE_DIR / \"vdt_results.json\", \"w\") as f:\n",
        "    json.dump({\"attributes\": attributes, \"vdt\": vdt, \"acc_baseline_500\": acc_baseline, \"acc_vdt_500\": acc_vdt}, f, indent=2)\n",
        "\n",
        "print(\"Saved VDT outputs and results to\", CACHE_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EDdIjd7V6la",
        "outputId": "10554878-435d-4908-874d-d492b174b343"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 20 attributes\n",
            "Loaded VDT entries: 37\n",
            "Total prompts: 740\n",
            "Encoding VDT prompts...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3406729752.py:81: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  final_class_embs = torch.tensor(final_class_embs)  # (37, 512)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VDT text embedding shape: torch.Size([740, 512])\n",
            "Final class embedding matrix: torch.Size([37, 512])\n",
            "Running VDT-enhanced zero-shot evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/7390 [00:00<35:50,  3.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DEBUG: oxford_pets/images/Abyssinian_1.jpg\n",
            " TRUE: abyssinian\n",
            " PRED: abyssinian  score= 0.3985186815261841\n",
            " PRED: bengal  score= 0.3210342526435852\n",
            " PRED: sphynx  score= 0.3201451599597931\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 2/7390 [00:00<33:43,  3.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DEBUG: oxford_pets/images/Abyssinian_10.jpg\n",
            " TRUE: abyssinian\n",
            " PRED: egyptian_mau  score= 0.28052714467048645\n",
            " PRED: abyssinian  score= 0.27945488691329956\n",
            " PRED: siamese  score= 0.27397793531417847\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 3/7390 [00:00<31:55,  3.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DEBUG: oxford_pets/images/Abyssinian_100.jpg\n",
            " TRUE: abyssinian\n",
            " PRED: abyssinian  score= 0.3075641393661499\n",
            " PRED: british_shorthair  score= 0.28348308801651\n",
            " PRED: bengal  score= 0.2733459770679474\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7390/7390 [33:32<00:00,  3.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=======================================\n",
            "VDT Zero-shot accuracy: 0.8525033829499323\n",
            "=======================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "VDT_PATH = \"vdt_attributes_sentences.json\"\n",
        "\n",
        "with open(VDT_PATH, \"r\") as f:\n",
        "    vdt_data = json.load(f)\n",
        "\n",
        "attributes = vdt_data[\"attributes\"]\n",
        "vdt = vdt_data[\"vdt\"]\n",
        "\n",
        "print(\"Loaded\", len(attributes), \"attributes\")\n",
        "print(\"Loaded VDT entries:\", len(vdt))\n",
        "\n",
        "PROMPT_TEMPLATE = \"a photo of a {classname}. {sentence}\"\n",
        "\n",
        "all_prompts = []\n",
        "idx_to_pair = []   \n",
        "\n",
        "for cls in classnames:\n",
        "    sentences = vdt[cls]\n",
        "    for j, sent in enumerate(sentences):\n",
        "        prompt = PROMPT_TEMPLATE.format(\n",
        "            classname=cls.replace(\"_\", \" \"),\n",
        "            sentence=sent\n",
        "        )\n",
        "        all_prompts.append(prompt)\n",
        "        idx_to_pair.append((cls, j))\n",
        "\n",
        "print(\"Total prompts:\", len(all_prompts))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_texts_clips(prompts):\n",
        "    BATCH = 64\n",
        "    all_embs = []\n",
        "    for i in range(0, len(prompts), BATCH):\n",
        "        batch = prompts[i:i+BATCH]\n",
        "        tokens = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "        feats = clip_model.get_text_features(**tokens)\n",
        "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
        "        all_embs.append(feats.cpu())\n",
        "    return torch.cat(all_embs, dim=0)\n",
        "\n",
        "print(\"Encoding VDT prompts...\")\n",
        "text_emb_vdt_all = encode_texts_clips(all_prompts)\n",
        "print(\"VDT text embedding shape:\", text_emb_vdt_all.shape)\n",
        "\n",
        "class_embs = {cls: [] for cls in classnames}\n",
        "\n",
        "for emb, (cls, attr_idx) in zip(text_emb_vdt_all, idx_to_pair):\n",
        "    class_embs[cls].append(emb.numpy())\n",
        "\n",
        "final_class_embs = []\n",
        "for cls in classnames:\n",
        "    arr = np.stack(class_embs[cls], axis=0).mean(axis=0)\n",
        "    arr = arr / np.linalg.norm(arr)\n",
        "    final_class_embs.append(arr)\n",
        "\n",
        "final_class_embs = torch.tensor(final_class_embs)  \n",
        "print(\"Final class embedding matrix:\", final_class_embs.shape)\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_image(img):\n",
        "    inputs = processor(images=[img], return_tensors=\"pt\").to(device)\n",
        "    feats = clip_model.get_image_features(**inputs)[0]\n",
        "    feats = feats / feats.norm()\n",
        "    return feats.cpu().numpy()\n",
        "\n",
        "def zero_shot_eval_vdt(items, class_emb_matrix, debug=3):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for idx, (imgpath, label) in enumerate(tqdm(items)):\n",
        "        img = Image.open(imgpath).convert(\"RGB\")\n",
        "        img_emb = encode_image(img)\n",
        "        sims = img_emb @ class_emb_matrix.numpy().T\n",
        "\n",
        "        pred = int(np.argmax(sims))\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "\n",
        "        if idx < debug:\n",
        "            print(\"\\nDEBUG:\", imgpath)\n",
        "            top3 = sims.argsort()[-3:][::-1]\n",
        "            print(\" TRUE:\", classnames[label])\n",
        "            for k in top3:\n",
        "                print(\" PRED:\", classnames[k], \" score=\", float(sims[k]))\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "print(\"Running VDT-enhanced zero-shot evaluation...\")\n",
        "vdt_acc = zero_shot_eval_vdt(items, final_class_embs)\n",
        "\n",
        "print(\"\\n=======================================\")\n",
        "print(\"VDT Zero-shot accuracy:\", vdt_acc)\n",
        "print(\"=======================================\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
